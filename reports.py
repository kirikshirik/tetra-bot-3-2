# utils/reports.py
import logging
from datetime import datetime, timedelta, time
from collections import Counter, defaultdict
from pytz import timezone

from aiogram.utils.markdown import escape_md
from aiogram import Bot

from config import (SCHEDULER_TIMEZONE, TOP_N_REASONS_FOR_SUMMARY,
                    PRODUCTION_SITES, LINES_SECTIONS, ADMIN_ROLE, PRODUCTION_SITE_EMOJIS)
from utils.storage import DataStorage

# –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –ø–ª–æ—â–∞–¥–∫–∏ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
SITE_NAME_TO_KEY = {v.lower(): k for k, v in PRODUCTION_SITES.items()}

def get_shift_time_range(shift_type: str) -> (datetime, datetime):
    tz = timezone(SCHEDULER_TIMEZONE)
    now_local = datetime.now(tz)
    time_08_00 = time(8, 0)
    time_20_00 = time(20, 0)

    if time_08_00 <= now_local.time() < time_20_00:
        current_start = now_local.replace(hour=8, minute=0, second=0, microsecond=0)
        current_end = now_local.replace(hour=20, minute=0, second=0, microsecond=0)
    else:
        if now_local.time() >= time_20_00:
            current_start = now_local.replace(hour=20, minute=0, second=0, microsecond=0)
            current_end = (now_local + timedelta(days=1)).replace(hour=8, minute=0, second=0, microsecond=0)
        else:
            current_start = (now_local - timedelta(days=1)).replace(hour=20, minute=0, second=0, microsecond=0)
            current_end = now_local.replace(hour=8, minute=0, second=0, microsecond=0)

    if shift_type == 'current':
        return current_start, current_end
    elif shift_type == 'previous':
        if current_start.time() == time_08_00:
            prev_end = current_start
            prev_start = (current_start - timedelta(days=1)).replace(hour=20, minute=0, second=0, microsecond=0)
        else:
            prev_end = current_start
            prev_start = current_start.replace(hour=8, minute=0, second=0, microsecond=0)
        return prev_start, prev_end

    return None, None


def calculate_shift_times(record_datetime: datetime) -> (str, str):
    tz = timezone(SCHEDULER_TIMEZONE)
    record_datetime_aware = record_datetime.astimezone(tz) if record_datetime.tzinfo else tz.localize(record_datetime)
    record_date = record_datetime_aware.date()
    record_time = record_datetime_aware.time()
    time_08_00 = time(8, 0)
    time_20_00 = time(20, 0)

    if time_08_00 <= record_time < time_20_00:
        start_dt = tz.localize(datetime.combine(record_date, time_08_00))
        end_dt = tz.localize(datetime.combine(record_date, time_20_00))
    elif record_time >= time_20_00:
        start_dt = tz.localize(datetime.combine(record_date, time_20_00))
        end_dt = tz.localize(datetime.combine(record_date + timedelta(days=1), time_08_00))
    else:
        start_dt = tz.localize(datetime.combine(record_date - timedelta(days=1), time_20_00))
        end_dt = tz.localize(datetime.combine(record_date, time_08_00))

    return start_dt.strftime("%Y-%m-%d %H:%M:%S"), end_dt.strftime("%Y-%m-%d %H:%M:%S")

def _parse_datetime_from_sheet(dt_string: str) -> datetime | None:
    """–ü—ã—Ç–∞–µ—Ç—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å –¥–∞—Ç–æ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –ø—Ä–æ–±—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
    formats_to_try = [
        "%Y-%m-%d %H:%M:%S",
        "%d.%m.%Y %H:%M:%S",
        "%Y/%m/%d %H:%M:%S",
    ]
    for fmt in formats_to_try:
        try:
            return datetime.strptime(dt_string, fmt)
        except ValueError:
            continue
    logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã-–≤—Ä–µ–º–µ–Ω–∏: '{dt_string}'")
    return None

async def get_downtime_report_for_period(start_dt: datetime, end_dt: datetime, storage: DataStorage):
    cache_status = ""
    if storage.downtime_cache.get("error"):
        cache_status += f"\n\n‚ö†Ô∏è **–ö—ç—à-–æ—à–∏–±–∫–∞: {storage.downtime_cache['error']}.**"
    if storage.is_cache_stale():
        cache_status += f"\n\n‚ö†Ô∏è **–î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã (–∫—ç—à —É—Å—Ç–∞—Ä–µ–ª).**"

    headers = storage.downtime_cache.get("headers")
    data_rows = storage.downtime_cache.get("data_rows")

    if not headers or data_rows is None:
        return {}, 0, 0, f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ—Å—Ç–æ—è—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.{cache_status}"

    try:
        required_cols = [
            "Timestamp_–∑–∞–ø–∏—Å–∏", "–ü–ª–æ—â–∞–¥–∫–∞", "–õ–∏–Ω–∏—è_–°–µ–∫—Ü–∏—è", "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è",
            "–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç", "–ü—Ä–∏—á–∏–Ω–∞_–ø—Ä–æ—Å—Ç–æ—è_–æ–ø–∏—Å–∞–Ω–∏–µ", "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è_–≥—Ä—É–ø–ø–∞",
            "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π_–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π_–∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞", "–ö—Ç–æ_–ø—Ä–∏–Ω—è–ª_–∑–∞—è–≤–∫—É_–ò–º—è", "–ö—Ç–æ_–∑–∞–≤–µ—Ä—à–∏–ª_—Ä–∞–±–æ—Ç—É_–≤_–≥—Ä—É–ø–ø–µ_–ò–º—è"
        ]
        idx_map = {col: headers.index(col) for col in required_cols}
    except ValueError as e:
        logging.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤ —Ç–∞–±–ª–∏—Ü–µ: {e}")
        error_message = f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: —Å—Ç–æ–ª–±–µ—Ü '{str(e).split()[0]}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–∞–±–ª–∏—Ü–µ."
        return {}, 0, 0, error_message

    downtimes_by_site = defaultdict(lambda: {'total_minutes': 0, 'entries': []})
    total_minutes_overall = 0
    record_count = 0
    tz = timezone(SCHEDULER_TIMEZONE)

    for row in data_rows:
        try:
            if len(row) <= max(idx_map.values()): continue
            
            record_timestamp_str = row[idx_map["Timestamp_–∑–∞–ø–∏—Å–∏"]]
            if not record_timestamp_str: continue
            
            record_dt = _parse_datetime_from_sheet(record_timestamp_str)
            if not record_dt: continue

            record_dt_aware = tz.localize(record_dt)

            if start_dt <= record_dt_aware < end_dt:
                record_count += 1
                site_name = row[idx_map['–ü–ª–æ—â–∞–¥–∫–∞']] # –ü–æ–ª—É—á–∞–µ–º "—á–∏—Å—Ç–æ–µ" –∏–º—è –±–µ–∑ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                line_section = escape_md(row[idx_map['–õ–∏–Ω–∏—è_–°–µ–∫—Ü–∏—è']])
                reason = escape_md(row[idx_map['–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è']])
                duration = int(row[idx_map["–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç"]] or 0)
                description = escape_md(row[idx_map["–ü—Ä–∏—á–∏–Ω–∞_–ø—Ä–æ—Å—Ç–æ—è_–æ–ø–∏—Å–∞–Ω–∏–µ"]])
                resp_group = escape_md(row[idx_map['–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è_–≥—Ä—É–ø–ø–∞']])
                accepted_by = escape_md(row[idx_map['–ö—Ç–æ_–ø—Ä–∏–Ω—è–ª_–∑–∞—è–≤–∫—É_–ò–º—è']])
                completed_by = escape_md(row[idx_map['–ö—Ç–æ_–∑–∞–≤–µ—Ä—à–∏–ª_—Ä–∞–±–æ—Ç—É_–≤_–≥—Ä—É–ø–ø–µ_–ò–º—è']])
                initiator_comment = escape_md(row[idx_map["–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π_–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π_–∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞"]])

                total_minutes_overall += duration
                downtimes_by_site[site_name]['total_minutes'] += duration

                entry_details = [
                    f"   ‚îî ‚öôÔ∏è **{line_section}: {reason} ({duration} –º–∏–Ω.)**",
                    f"         ‚îî üóíÔ∏è –û–ø–∏—Å–∞–Ω–∏–µ: _{description}_",
                    f"         ‚îî üë• –û—Ç–≤. –≥—Ä—É–ø–ø–∞: {resp_group}"
                ]
                if accepted_by:
                    entry_details.append(f"         ‚îî üë®‚Äçüíª –ü—Ä–∏–Ω—è–ª: {accepted_by}")
                if completed_by:
                    entry_details.append(f"         ‚îî üë®‚Äçüíª –†–∞–±–æ—Ç—É –≤ –≥—Ä—É–ø–ø–µ –∑–∞–≤–µ—Ä—à–∏–ª: {completed_by}")

                if initiator_comment and "–ë–µ–∑ –¥–æ–ø. –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è" not in initiator_comment:
                    entry_details.append(f"         ‚îî üó£Ô∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞: _{initiator_comment}_")
                
                downtimes_by_site[site_name]['entries'].append("\n".join(entry_details))

        except (ValueError, IndexError) as e:
            logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {row}. –û—à–∏–±–∫–∞: {e}")
            continue

    if record_count == 0:
        no_records_message = f"‚úÖ **–û—Ç—á–µ—Ç –∑–∞ —Å–º–µ–Ω—É**\n–ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∑–∞ —Å–º–µ–Ω—É —Å {start_dt.strftime('%d.%m.%Y %H:%M')} –ø–æ {end_dt.strftime('%d.%m.%Y %H:%M')}.{cache_status}"
        return {}, 0, 0, no_records_message

    reports_by_site_dict = {}
    for site_name_from_sheet, data in sorted(downtimes_by_site.items()):
        
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–∏–≤–æ–¥–∏–º –∏–º—è –ø–ª–æ—â–∞–¥–∫–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        site_key = SITE_NAME_TO_KEY.get(site_name_from_sheet.strip().lower())
        emoji = PRODUCTION_SITE_EMOJIS.get(site_key, '‚ö™Ô∏è') # –ë–µ–ª—ã–π –∫—Ä—É–≥ - —ç–º–æ–¥–∑–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        site_total_minutes = data['total_minutes']
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º Markdown –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ø–ª–æ—â–∞–¥–∫–∏ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
        escaped_site_name = escape_md(site_name_from_sheet)
        report_parts = [f"{emoji} **{escaped_site_name} –û–±—â–µ–µ –≤—Ä–µ–º—è –ø—Ä–æ—Å—Ç–æ—è: {site_total_minutes} –º–∏–Ω—É—Ç.**"]
        report_parts.extend(data['entries'])
        reports_by_site_dict[site_name_from_sheet] = "\n".join(report_parts)

    return reports_by_site_dict, total_minutes_overall, record_count, cache_status


async def generate_admin_shift_summary(start_dt: datetime, end_dt: datetime, storage: DataStorage):
    headers = storage.downtime_cache.get("headers")
    data_rows = storage.downtime_cache.get("data_rows")

    if not headers or data_rows is None: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤–æ–¥–∫–∏."

    try:
        idx_map = {col: headers.index(col) for col in ["Timestamp_–∑–∞–ø–∏—Å–∏", "–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç", "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è"]}
    except ValueError as e: return f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: —Å—Ç–æ–ª–±–µ—Ü '{str(e).split()[0]}' –Ω–µ –Ω–∞–π–¥–µ–Ω."
    
    total_minutes = 0
    reason_counts = Counter()
    tz = timezone(SCHEDULER_TIMEZONE)

    for row in data_rows:
        try:
            if len(row) <= max(idx_map.values()): continue
            record_timestamp_str = row[idx_map["Timestamp_–∑–∞–ø–∏—Å–∏"]]
            if not record_timestamp_str: continue
            
            record_dt = _parse_datetime_from_sheet(record_timestamp_str)
            if not record_dt: continue

            record_dt_aware = tz.localize(record_dt)

            if start_dt <= record_dt_aware < end_dt:
                duration = int(row[idx_map["–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç"]] or 0)
                reason = row[idx_map["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è"]] or "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
                total_minutes += duration
                reason_counts[reason] += duration
        except (ValueError, IndexError) as e:
            logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–≤–æ–¥–∫–∏: {row}. –û—à–∏–±–∫–∞: {e}")
            continue

    if total_minutes == 0:
        return f"–ó–∞ —Å–º–µ–Ω—É ({start_dt.strftime('%H:%M')}-{end_dt.strftime('%H:%M')}) –ø—Ä–æ—Å—Ç–æ–µ–≤ –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ."

    hours, minutes = divmod(total_minutes, 60)
    top_reasons_list = reason_counts.most_common(TOP_N_REASONS_FOR_SUMMARY)
    top_reasons = [f"- {escape_md(r)} ({m} –º–∏–Ω.)" for r, m in top_reasons_list]
    summary = (f"**–°–≤–æ–¥–∫–∞ –∑–∞ —Å–º–µ–Ω—É ({start_dt.strftime('%H:%M %d.%m')})**\n\n"
               f"–û–±—â–∏–π –ø—Ä–æ—Å—Ç–æ–π: **{hours} —á {minutes} –º–∏–Ω.**\n\n"
               f"**–¢–æ–ø-{len(top_reasons)} –ø—Ä–∏—á–∏–Ω—ã:**\n" + "\n".join(top_reasons))
    return summary


async def generate_line_status_report(storage: DataStorage):
    report_lines = ["**–°—Ç–∞—Ç—É—Å –ª–∏–Ω–∏–π –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç:**"]
    for site_key, site_name in PRODUCTION_SITES.items():
        if site_key not in LINES_SECTIONS: continue
        
        emoji = PRODUCTION_SITE_EMOJIS.get(site_key, '‚ö™Ô∏è')
        report_lines.append(f"\n{emoji} **{escape_md(site_name)}**")
        
        for line_key, line_name in LINES_SECTIONS[site_key].items():
            line_tuple = (site_name, line_name)
            if line_tuple in storage.active_downtimes:
                reason = storage.active_downtimes[line_tuple]
                report_lines.append(f"   üî¥ {escape_md(line_name)}: **–ü–†–û–°–¢–û–ô** ({escape_md(reason)})")
            else:
                report_lines.append(f"   üü¢ {escape_md(line_name)}: –†–∞–±–æ—Ç–∞–µ—Ç")
    return "\n".join(report_lines)


async def scheduled_line_status_report(bot: Bot, storage: DataStorage):
    logging.info("SCHEDULER: –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫—É –æ—Ç—á–µ—Ç–∞ –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π.")
    admin_ids = [uid for uid, role in storage.user_roles.items() if role == ADMIN_ROLE]
    if not admin_ids:
        logging.warning("SCHEDULER: –ù–µ—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π.")
        return
    report_text = await generate_line_status_report(storage)
    for admin_id in admin_ids:
        try:
            await bot.send_message(int(admin_id), report_text, parse_mode="Markdown")
        except Exception as e:
            logging.error(f"SCHEDULER: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ—Ç—á–µ—Ç –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π –∞–¥–º–∏–Ω—É {admin_id}: {e}")
    logging.info(f"SCHEDULER: –û—Ç—á–µ—Ç –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {len(admin_ids)} –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º.")